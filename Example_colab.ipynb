{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lorenzosteccanella/HRL-MDP/blob/main/Example_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "174330379b8a7052"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A simple example on how to learn a representation on the Nine Rooms environment"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f38cbc8e84fb9a5b"
   },
   "id": "f38cbc8e84fb9a5b"
  },
  {
   "cell_type": "code",
   "source": [
    "# only required to run this notebook on COLAB\n",
    "\n",
    "!git clone https://github.com/lorenzosteccanella/HRL-MDP.git\n",
    "!cd HRL-MDP && ls && pip install -r requirements.txt\n",
    "%cd HRL-MDP"
   ],
   "metadata": {
    "id": "dvrBH6ssrVK4"
   },
   "id": "dvrBH6ssrVK4",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To supress old gym deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', message='Parameters to load are deprecated.*')\n",
    "\n",
    "import numpy as np\n",
    "from utils import collect_trajectories, representation_score, wandb_plot\n",
    "from model import SoftClusterNetwork\n",
    "from torch import optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "2b9e16ace306aab2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = {\n",
    "    \"seed\": 0,\n",
    "    \"env\": \"MiniGrid-NineRoomsDet-v0\",\n",
    "    \"load_data\": False,\n",
    "    \"p_random_action\": 0,\n",
    "    \"max_len_episode\": 100,\n",
    "    \"pos_or_image\": \"image\",\n",
    "    \"n_episodes_env\": 1000,\n",
    "    \"n_abstract_states\": 9,\n",
    "    \"width\": 19,\n",
    "    \"height\": 19,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 2000,\n",
    "    \"batch_size\": 32,\n",
    "    \"wl1\": 1,\n",
    "    \"wl2\": 0.4,\n",
    "    \"wl3\": 0.1\n",
    "}"
   ],
   "id": "9f96b211a7cda392"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# First let's collect some random trajectory data\n",
    "memory, trajectories_dataset, print_states, annotations = collect_trajectories(config)\n"
   ],
   "id": "2f94cee2fbf414cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# set seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "# # # Training the network\n",
    "network = SoftClusterNetwork(config[\"n_abstract_states\"], config[\"width\"], config[\"height\"])\n",
    "optimizer = optim.Adam(network.parameters(), lr=config[\"lr\"])\n"
   ],
   "id": "921a22222ed6abc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "losses = []\n",
    "for i in range(config[\"epochs\"]):\n",
    "    network.train()\n",
    "    idx, batch_x1, batch_x2, b_is_weights = memory.sample(config[\"batch_size\"])\n",
    "    x1 = torch.stack(batch_x1)\n",
    "    x2 = torch.stack(batch_x2)\n",
    "    z1 = network.pred(x1, 1)\n",
    "    z2 = network.pred(x2, 1)\n",
    "    compression_loss = ((-(z1 * z2.log())).sum(axis=1)).mean(axis=0)\n",
    "    compression_loss = compression_loss / config[\"batch_size\"]\n",
    "    entropy_loss = (z1.mean(dim=0) * (z1.mean(dim=0).log())).sum()\n",
    "    det_entropy_loss = (- (z1 * z1.log()).sum(dim=1)).mean()\n",
    "    loss = config[\"wl1\"] * compression_loss + config[\"wl2\"] * entropy_loss + config[\"wl3\"] * det_entropy_loss\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i}, Loss: {loss.item(), compression_loss.item(), entropy_loss.item(), det_entropy_loss.item()}\")\n",
    "        error, squared_error, abs_error = representation_score(config, network.eval())\n",
    "        print(f\"Error: {error}, Squared Error: {squared_error}, Abs Error: {abs_error}\")\n",
    "        fig = wandb_plot(print_states, annotations, network.eval(), d=2)\n",
    "        # Draw figure on canvas\n",
    "        fig.canvas.draw()\n",
    "        plt.show()\n",
    "\n",
    "        # Create a new figure for losses\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # moving windows on the losses\n",
    "        losses_to_plot = np.convolve(losses, np.ones(100)/100, mode='valid')\n",
    "        # plot the losses graph with labels and title\n",
    "        plt.plot(losses_to_plot, color='blue', linewidth=2)\n",
    "        plt.xlabel('Training Steps (Moving Average Window=100)')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Replace gradient clamping with norm clipping\n",
    "    torch.nn.utils.clip_grad_norm_(network.parameters(), max_norm=0.1)\n",
    "    optimizer.step()"
   ],
   "id": "69e16aca23912a52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
